{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMszDSFVkjZzDIjKSHER/s6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DivyamThapliyal07/Projects/blob/main/smartphone_price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LaP-YFqpB6qd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"/content/Smartphones_cleaned_dataset.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "QiTQIY4ZCJ1q",
        "outputId": "7e256b47-b727-4984-8689-30f595ad0620"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Smartphones_cleaned_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-3243586904.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Smartphones_cleaned_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Smartphones_cleaned_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "gpqKLEUMCSpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "I2lWGFRBCVzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "rfuzxSiOCduy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"rating\"]=df[\"rating\"].fillna(df[\"rating\"].mean)"
      ],
      "metadata": {
        "id": "KkmNu1TGCl22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"num_cores\"]=df[\"num_cores\"].fillna(df[\"num_cores\"].mode)"
      ],
      "metadata": {
        "id": "OkQfFj3WCxzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"processor_speed\"]=df[\"processor_speed\"].fillna(df[\"processor_speed\"].mean)"
      ],
      "metadata": {
        "id": "0qhIs2PQDAaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"processor_brand\"]=df[\"processor_brand\"].fillna(df[\"processor_brand\"].mode)"
      ],
      "metadata": {
        "id": "georj0CoDHFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"primary_camera_front\"]=df[\"primary_camera_front\"].fillna(df[\"primary_camera_front\"].mean)"
      ],
      "metadata": {
        "id": "9f7qdegVDW3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"os\"]=df[\"os\"].fillna(df[\"os\"].mode)"
      ],
      "metadata": {
        "id": "Ab92RYjfDkQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(\"extended_upto\", axis=1)"
      ],
      "metadata": {
        "id": "4pKN6LFEDrsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Bop9-qxwD22X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "HP_1RqwOD34R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"battery_capacity\"]=df[\"battery_capacity\"].fillna(df[\"battery_capacity\"].mean)"
      ],
      "metadata": {
        "id": "cGkjDHq_EALA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: figure out a way for null vlaues now, oi ahve done few\n",
        "\n",
        "# Need to call the methods .mean() and .mode()\n",
        "df= pd.read_csv(\"/content/Smartphones_cleaned_dataset.csv\") # Reload the dataframe\n",
        "\n",
        "df[\"rating\"] = df[\"rating\"].fillna(df[\"rating\"].mean())\n",
        "df[\"num_cores\"] = df[\"num_cores\"].fillna(df[\"num_cores\"].mode()[0]) # mode can return multiple values, take the first\n",
        "df[\"processor_speed\"] = df[\"processor_speed\"].fillna(df[\"processor_speed\"].mean())\n",
        "df[\"processor_brand\"] = df[\"processor_brand\"].fillna(df[\"processor_brand\"].mode()[0])\n",
        "df[\"primary_camera_front\"] = df[\"primary_camera_front\"].fillna(df[\"primary_camera_front\"].mean())\n",
        "df[\"os\"] = df[\"os\"].fillna(df[\"os\"].mode()[0])\n",
        "df[\"battery_capacity\"] = df[\"battery_capacity\"].fillna(df[\"battery_capacity\"].mean())\n",
        "\n",
        "# Drop the 'extended_upto' column again as the dataframe was reloaded\n",
        "df = df.drop(\"extended_upto\", axis=1)\n",
        "\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "3SYI4usTEFcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: should we re,move fast charging\n",
        "\n",
        "if 'fast_charging' in df.columns:\n",
        "    df = df.drop(\"fast_charging\", axis=1)\n"
      ],
      "metadata": {
        "id": "f0GK6wtgExJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "edFZAhvhE7js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "QrpxyyL1Jfxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: remove the unmamed\n",
        "\n",
        "# Check for unnamed columns and drop them\n",
        "unnamed_cols = [col for col in df.columns if col.startswith('Unnamed:')]\n",
        "if unnamed_cols:\n",
        "    df = df.drop(columns=unnamed_cols)\n",
        "\n",
        "df.head()\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "iVamd8B7Jriz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s4Ied-XrFMMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.get_dummies(df,columns=[\"brand_name\", \"model\", \"processor_brand\",\"os\"])"
      ],
      "metadata": {
        "id": "KIxMjY9PE9d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "8qZ-SmSFFM7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "7IbOzm8VIOvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
      ],
      "metadata": {
        "id": "XUnZJpr3IYZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ],
      "metadata": {
        "id": "mwJcj2-bIgyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in 'num_front_cameras' with the mean\n",
        "df['num_front_cameras'] = df['num_front_cameras'].fillna(df['num_front_cameras'].mean())\n",
        "\n",
        "# Verify that there are no more missing values in 'num_front_cameras'\n",
        "print(\"Missing values after imputation:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "bZCmoxU6KAVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= df.drop(\"price\", axis=1)\n",
        "y= df[\"price\"]"
      ],
      "metadata": {
        "id": "HOlE2EGCInxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "d0daW9KEItMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models=[LinearRegression(), Lasso(alpha=0.1), Ridge(alpha=0.1)]"
      ],
      "metadata": {
        "id": "x1oCwngPI1cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models:\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred=model.predict(X_test)\n",
        "  y_residuals=y_test -y_pred\n",
        "  mean_squared_error(y_test, y_pred)\n",
        "  mean_absolute_error(y_test,y_pred)\n",
        "  r2_score(y_test, y_pred)\n",
        "  print(f\"Model: {model.__class__.__name__}\")\n",
        "  print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
        "  print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}\")\n",
        "  print(f\"R-squared: {r2_score(y_test, y_pred)}\")\n",
        "  print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "c5XQtac3I8Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: perfect now i wanna improve the accuracy like by hypertuning\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Define a more comprehensive list of models to hyperparameter tune\n",
        "models_to_tune = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'Lasso': Lasso(),\n",
        "    'Ridge': Ridge(),\n",
        "    'RandomForestRegressor': RandomForestRegressor(random_state=42),\n",
        "    'GradientBoostingRegressor': GradientBoostingRegressor(random_state=42),\n",
        "    'SVR': SVR(),\n",
        "    'XGBRegressor': xgb.XGBRegressor(random_state=42),\n",
        "    'LGBMRegressor': lgb.LGBMRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Define hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    'LinearRegression': {}, # No hyperparameters for basic Linear Regression\n",
        "    'Lasso': {\n",
        "        'alpha': [0.001, 0.01, 0.1, 1, 10]\n",
        "    },\n",
        "    'Ridge': {\n",
        "        'alpha': [0.001, 0.01, 0.1, 1, 10]\n",
        "    },\n",
        "    'RandomForestRegressor': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'GradientBoostingRegressor': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 4, 5],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'SVR': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'epsilon': [0.01, 0.1, 0.5],\n",
        "        'kernel': ['linear', 'rbf']\n",
        "    },\n",
        "    'XGBRegressor': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.7, 0.8, 0.9],\n",
        "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
        "    },\n",
        "    'LGBMRegressor': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'num_leaves': [31, 50, 100],\n",
        "        'max_depth': [-1, 10, 20],\n",
        "        'min_child_samples': [20, 30, 50]\n",
        "    }\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Iterate through each model and perform hyperparameter tuning using GridSearchCV\n",
        "for model_name, model in models_to_tune.items():\n",
        "    print(f\"Tuning {model_name}...\")\n",
        "    param_grid = param_grids[model_name]\n",
        "\n",
        "    # Use GridSearchCV for a systematic search\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "    # You could also use RandomizedSearchCV for a faster search on larger grids\n",
        "    # random_search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=5, scoring='r2', random_state=42, n_jobs=-1)\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results[model_name] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'mse': mse,\n",
        "        'mae': mae,\n",
        "        'r2': r2\n",
        "    }\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Mean Squared Error: {mse}\")\n",
        "    print(f\"Mean Absolute Error: {mae}\")\n",
        "    print(f\"R-squared: {r2}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# You can now analyze the results dictionary to compare the performance of tuned models\n",
        "print(\"\\nOverall Results:\")\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}: R-squared = {metrics['r2']:.4f}, Best Params = {metrics['best_params']}\")\n",
        "\n",
        "# Find the model with the best R-squared score\n",
        "best_performing_model = max(results, key=lambda k: results[k]['r2'])\n",
        "print(f\"\\nBest performing model is: {best_performing_model} with R-squared = {results[best_performing_model]['r2']:.4f}\")\n"
      ],
      "metadata": {
        "id": "4QmK06VrKKOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "param_grids= {'RandomForestRegressor':{\n",
        "              'n_estimators':[100, 200, 300],\n",
        "              'max_depth':[None,10, 20,30],\n",
        "              'min_samples_split':[2, 5, 10],\n",
        "              'min_samples_leaf':[1,2,4]\n",
        "                                       }\n",
        "              }"
      ],
      "metadata": {
        "id": "mv8fmnQ3L3-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search=GridSearchCV(RandomForestRegressor(random_state=42),param_grids['RandomForestRegressor'], cv=5, scoring='r2', n_jobs=1)"
      ],
      "metadata": {
        "id": "pu631VsbM4H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3ZJxm3DrNzon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_best=grid_search.predict(X_test)"
      ],
      "metadata": {
        "id": "GMPThz0GN8Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: evaluate y_pred and grid search in last cells\n",
        "\n",
        "mse_best = mean_squared_error(y_test, y_pred_best)\n",
        "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
        "r2_best = r2_score(y_test, y_pred_best)\n",
        "\n",
        "print(\"Evaluation of the best model found by GridSearchCV:\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Mean Squared Error: {mse_best}\")\n",
        "print(f\"Mean Absolute Error: {mae_best}\")\n",
        "print(f\"R-squared: {r2_best}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"\\nGrid Search results:\")\n",
        "grid_search.cv_results_"
      ],
      "metadata": {
        "id": "IpeoH1fAVuu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Visualize the distribution of the target variable ('price')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['price'], kde=True)\n",
        "plt.title('Distribution of Price')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Visualize correlations between features and the target variable\n",
        "# Select numerical columns for correlation\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "# Drop columns with high cardinality or that are dummy variables for correlation matrix if too large\n",
        "# Adjust this list based on your needs if the matrix is too large\n",
        "corr_cols = [col for col in numerical_cols if col not in ['model', 'brand_name', 'processor_brand', 'os'] and 'Unnamed' not in col]\n",
        "\n",
        "if len(corr_cols) > 1:\n",
        "    correlation_matrix = df[corr_cols].corr()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title('Correlation Matrix of Numerical Features')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Not enough numerical columns to plot a correlation matrix.\")\n",
        "\n",
        "\n",
        "if 'y_pred_best' in locals():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred_best, alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--') # Diagonal line\n",
        "    plt.xlabel('Actual Price')\n",
        "    plt.ylabel('Predicted Price')\n",
        "    plt.title('Actual vs. Predicted Price (Best Tuned Model)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Run the GridSearchCV tuning section first to get 'y_pred_best' for visualization.\")"
      ],
      "metadata": {
        "id": "8ZPFgeJbavz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Check if 'y_pred_best' is available before plotting\n",
        "if 'y_pred_best' in locals():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred_best, alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "    plt.xlabel('Actual Price')\n",
        "    plt.ylabel('Predicted Price')\n",
        "    plt.title('Actual vs. Predicted Price (Best Tuned Model)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Run the GridSearchCV tuning section first to get 'y_pred_best' for visualization.\")"
      ],
      "metadata": {
        "id": "pS9_uFPxa0NN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}